{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:52:11.726121900Z",
     "start_time": "2025-11-03T13:52:11.714125400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from src.model import vit_tiny_patch16_224"
   ],
   "id": "5d2b8fa9539601cf",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:52:12.280576600Z",
     "start_time": "2025-11-03T13:52:12.157150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_model = vit_tiny_patch16_224(num_classes=10)\n",
    "checkpoint = torch.load('best_model.pth', map_location=\"cpu\", weights_only=False)\n",
    "trained_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "trained_model.eval()"
   ],
   "id": "b811e953fca112bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbedding(\n",
       "    (projection): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerEncoder(\n",
       "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): FeedForward(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:52:13.231386100Z",
     "start_time": "2025-11-03T13:52:13.208562100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CIFAR10DATASET(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Path to CIFAR-10 data directory\n",
    "            transform: Optional transform to apply to images\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load data and labels\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load test batch\n",
    "        test_file = os.path.join(data_dir, 'test_batch')\n",
    "        with open(test_file, 'rb') as f:\n",
    "            test_dict = pickle.load(f, encoding='bytes')\n",
    "        self.data = test_dict[b'data']  # Shape: (10000, 3072)\n",
    "        self.labels = test_dict[b'labels']\n",
    "\n",
    "        # Reshape data from (N, 3072) to (N, 3, 32, 32)\n",
    "        self.data = self.data.reshape(-1, 3, 32, 32)\n",
    "        # Convert from [0, 255] to [0, 1]\n",
    "        self.data = self.data.astype(np.float32) / 255.0\n",
    "\n",
    "        # Load label names\n",
    "        meta_file = os.path.join(data_dir, 'batches.meta')\n",
    "        with open(meta_file, 'rb') as f:\n",
    "            meta_dict = pickle.load(f, encoding='bytes')\n",
    "        self.label_names = [name.decode('utf-8') for name in meta_dict[b'label_names']]\n",
    "\n",
    "        print(f\"Loaded test data: {len(self.data)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.from_numpy(self.data[idx])  # Shape: (3, 32, 32)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ],
   "id": "e3e73c2a323644d3",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:01:06.919016100Z",
     "start_time": "2025-11-03T14:01:06.901890500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CIFAR100DATASET(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Path to CIFAR-10 data directory\n",
    "            transform: Optional transform to apply to images\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load data and labels\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load test batch\n",
    "        test_file = os.path.join(data_dir, 'test')\n",
    "        with open(test_file, 'rb') as f:\n",
    "            test_dict = pickle.load(f, encoding='bytes')\n",
    "        self.data = test_dict[b'data']  # Shape: (10000, 3072)\n",
    "        self.labels = test_dict[b'coarse_labels']\n",
    "\n",
    "        # Reshape data from (N, 3072) to (N, 3, 32, 32)\n",
    "        self.data = self.data.reshape(-1, 3, 32, 32)\n",
    "        # Convert from [0, 255] to [0, 1]\n",
    "        self.data = self.data.astype(np.float32) / 255.0\n",
    "\n",
    "        # Load label names\n",
    "        meta_file = os.path.join(data_dir, 'meta')\n",
    "        with open(meta_file, 'rb') as f:\n",
    "            meta_dict = pickle.load(f, encoding='bytes')\n",
    "        print(meta_dict.keys())\n",
    "        self.label_names = [name.decode('utf-8') for name in meta_dict[b'coarse_label_names']]\n",
    "\n",
    "        print(f\"Loaded test data: {len(self.data)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.from_numpy(self.data[idx])  # Shape: (3, 32, 32)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ],
   "id": "621fc9f7b43351da",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:08:43.119760900Z",
     "start_time": "2025-11-03T14:08:43.096749300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader)\n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss / (batch_idx + 1),\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ],
   "id": "d9fa8b4e33255ed",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:08:43.677623500Z",
     "start_time": "2025-11-03T14:08:43.579536500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = CIFAR100DATASET('cifar-100', transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.4914, 0.4822, 0.4465],\n",
    "                std=[0.2470, 0.2435, 0.2616]\n",
    "            ),\n",
    "        ]))"
   ],
   "id": "a200bff599a10e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([b'fine_label_names', b'coarse_label_names'])\n",
      "Loaded test data: 10000 images\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:08:44.057844900Z",
     "start_time": "2025-11-03T14:08:44.030847400Z"
    }
   },
   "cell_type": "code",
   "source": "criterion = nn.CrossEntropyLoss()",
   "id": "e9dec21f5eb13476",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:08:44.485426300Z",
     "start_time": "2025-11-03T14:08:44.471242300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_loader =  DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )"
   ],
   "id": "8c306ba0ce5cbde4",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:08:44.876873600Z",
     "start_time": "2025-11-03T14:08:44.862706100Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "4e592b714fe6b7a1",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:08:45.716535500Z",
     "start_time": "2025-11-03T14:08:45.609928800Z"
    }
   },
   "cell_type": "code",
   "source": "val_loss, val_acc = validate(trained_model, val_loader, criterion, device)",
   "id": "547ae5a1e06d0760",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.1700,  1.1700,  1.1700,  ...,  1.5669,  1.5669,  1.5669],\n",
      "          [ 1.1700,  1.1700,  1.1700,  ...,  1.5669,  1.5669,  1.5669],\n",
      "          [ 1.1700,  1.1700,  1.1700,  ...,  1.5669,  1.5669,  1.5669],\n",
      "          ...,\n",
      "          [-0.7193, -0.7193, -0.7193,  ...,  0.8048,  0.8048,  0.8048],\n",
      "          [-0.7193, -0.7193, -0.7193,  ...,  0.8048,  0.8048,  0.8048],\n",
      "          [-0.7193, -0.7193, -0.7193,  ...,  0.8048,  0.8048,  0.8048]],\n",
      "\n",
      "         [[ 1.4823,  1.4823,  1.4823,  ...,  1.7883,  1.7883,  1.7883],\n",
      "          [ 1.4823,  1.4823,  1.4823,  ...,  1.7883,  1.7883,  1.7883],\n",
      "          [ 1.4823,  1.4823,  1.4823,  ...,  1.7883,  1.7883,  1.7883],\n",
      "          ...,\n",
      "          [-0.6114, -0.6114, -0.6114,  ...,  0.6609,  0.6609,  0.6609],\n",
      "          [-0.6114, -0.6114, -0.6114,  ...,  0.6609,  0.6609,  0.6609],\n",
      "          [-0.6114, -0.6114, -0.6114,  ...,  0.6609,  0.6609,  0.6609]],\n",
      "\n",
      "         [[ 2.0259,  2.0259,  2.0259,  ...,  2.0709,  2.0709,  2.0709],\n",
      "          [ 2.0259,  2.0259,  2.0259,  ...,  2.0709,  2.0709,  2.0709],\n",
      "          [ 2.0259,  2.0259,  2.0259,  ...,  2.0709,  2.0709,  2.0709],\n",
      "          ...,\n",
      "          [ 0.0621,  0.0621,  0.0621,  ...,  1.0365,  1.0365,  1.0365],\n",
      "          [ 0.0621,  0.0621,  0.0621,  ...,  1.0365,  1.0365,  1.0365],\n",
      "          [ 0.0621,  0.0621,  0.0621,  ...,  1.0365,  1.0365,  1.0365]]]]) tensor([10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 10 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[73], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m val_loss, val_acc \u001B[38;5;241m=\u001B[39m \u001B[43mvalidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrained_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[68], line 16\u001B[0m, in \u001B[0;36mvalidate\u001B[1;34m(model, val_loader, criterion, device)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m     15\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(images)\n\u001B[1;32m---> 16\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Statistics\u001B[39;00m\n\u001B[0;32m     19\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\COMP3314-2025-2026-_Group16_ViT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1774\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1775\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\COMP3314-2025-2026-_Group16_ViT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1782\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1783\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1784\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1785\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1788\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1789\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\COMP3314-2025-2026-_Group16_ViT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1385\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m   1383\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m   1384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Runs the forward pass.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1386\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1388\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1389\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1390\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1391\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1392\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\COMP3314-2025-2026-_Group16_ViT\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:3458\u001B[0m, in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3457\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3458\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3459\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3460\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3461\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3462\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3463\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3464\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3465\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mIndexError\u001B[0m: Target 10 is out of bounds."
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:53:49.514480200Z",
     "start_time": "2025-11-03T13:53:49.468542100Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")",
   "id": "ea07a9ca4b4e51ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 0.9240 | Val Acc: 83.02%\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "be89a2e488a871c2"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
