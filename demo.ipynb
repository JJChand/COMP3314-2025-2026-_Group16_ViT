{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision Transformer (ViT) Implementation Demo\n",
        "## COMP3314 Group 16 - Reproducing \"An Image is Worth 16x16 Words\"\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. **Training Visualization** - Analyze CIFAR-100 training progress\n",
        "2. **Attention Visualization** - Understand what the model learns\n",
        "3. **Transfer Learning** - Fine-tune CIFAR-100 model on CIFAR-10\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Part 1: Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "# Import project modules\n",
        "import sys\n",
        "sys.path.append('./src')\n",
        "from model import VisionTransformer, vit_small_patch16_224\n",
        "from train import CIFAR100Dataset, get_transforms, TrainingHistory\n",
        "from utils import load_model, CIFAR100_CLASSES\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìä Part 2: Training Visualization (CIFAR-100)\n",
        "\n",
        "Analyze the training progress of our ViT-Small model on CIFAR-100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Define Visualizer Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ViTVisualizer:\n",
        "    \"\"\"Visualizer for Vision Transformer training progress\"\"\"\n",
        "    \n",
        "    def __init__(self, checkpoint_dir='./src/checkpoints'):\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.history = None\n",
        "    \n",
        "    def load_history_from_json(self, json_path=None):\n",
        "        \"\"\"Load training history from JSON file\"\"\"\n",
        "        if json_path is None:\n",
        "            json_path = os.path.join(self.checkpoint_dir, 'training_history.json')\n",
        "        \n",
        "        print(f\"Loading history from: {json_path}\")\n",
        "        history_obj = TrainingHistory()\n",
        "        history_obj.load(json_path)\n",
        "        \n",
        "        self.history = {\n",
        "            'epochs': history_obj.epochs,\n",
        "            'train_losses': history_obj.train_losses,\n",
        "            'train_accs': history_obj.train_accs,\n",
        "            'val_losses': history_obj.val_losses,\n",
        "            'val_accs': history_obj.val_accs,\n",
        "            'learning_rates': history_obj.learning_rates\n",
        "        }\n",
        "        return self.history\n",
        "    \n",
        "    def plot_training_progress(self, save_path=None):\n",
        "        \"\"\"Plot training progress with 4 subplots\"\"\"\n",
        "        if not self.history or not self.history.get('epochs'):\n",
        "            print(\"No training history available\")\n",
        "            return\n",
        "        \n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        epochs = self.history['epochs']\n",
        "        \n",
        "        # 1. Loss curves\n",
        "        ax1.plot(epochs, self.history['train_losses'], 'b-', label='Train Loss', linewidth=2)\n",
        "        ax1.plot(epochs, self.history['val_losses'], 'r-', label='Val Loss', linewidth=2)\n",
        "        ax1.set_xlabel('Epoch', fontsize=12)\n",
        "        ax1.set_ylabel('Loss', fontsize=12)\n",
        "        ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 2. Accuracy curves\n",
        "        ax2.plot(epochs, self.history['train_accs'], 'b-', label='Train Acc', linewidth=2)\n",
        "        ax2.plot(epochs, self.history['val_accs'], 'r-', label='Val Acc', linewidth=2)\n",
        "        ax2.set_xlabel('Epoch', fontsize=12)\n",
        "        ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "        ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 3. Learning rate schedule\n",
        "        ax3.plot(epochs, self.history['learning_rates'], 'g-', linewidth=2)\n",
        "        ax3.set_xlabel('Epoch', fontsize=12)\n",
        "        ax3.set_ylabel('Learning Rate', fontsize=12)\n",
        "        ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "        ax3.set_yscale('log')\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 4. Overfitting analysis\n",
        "        overfit_gap = [train - val for train, val in \n",
        "                      zip(self.history['train_accs'], self.history['val_accs'])]\n",
        "        ax4.plot(epochs, overfit_gap, 'orange', linewidth=2)\n",
        "        ax4.set_xlabel('Epoch', fontsize=12)\n",
        "        ax4.set_ylabel('Train-Val Gap (%)', fontsize=12)\n",
        "        ax4.set_title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "        ax4.axhline(y=np.mean(overfit_gap), color='red', linestyle='--', \n",
        "                   label=f'Avg: {np.mean(overfit_gap):.2f}%')\n",
        "        ax4.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"Saved: {save_path}\")\n",
        "        plt.show()\n",
        "        return fig\n",
        "    \n",
        "    def print_summary(self):\n",
        "        \"\"\"Print training summary\"\"\"\n",
        "        if not self.history or not self.history.get('epochs'):\n",
        "            return\n",
        "        \n",
        "        train_accs = self.history['train_accs']\n",
        "        val_accs = self.history['val_accs']\n",
        "        \n",
        "        print(\"=\"*60)\n",
        "        print(\"CIFAR-100 TRAINING SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Final Train Accuracy:  {train_accs[-1]:.2f}%\")\n",
        "        print(f\"Final Val Accuracy:    {val_accs[-1]:.2f}%\")\n",
        "        print(f\"Best Val Accuracy:     {max(val_accs):.2f}% (Epoch {val_accs.index(max(val_accs))+1})\")\n",
        "        print(f\"Overfitting Gap:       {train_accs[-1] - val_accs[-1]:.2f}%\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "print(\"ViTVisualizer class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Load and Visualize CIFAR-100 Training History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizer and load history\n",
        "visualizer = ViTVisualizer(checkpoint_dir='./src/checkpoints')\n",
        "visualizer.load_history_from_json('./src/checkpoints/training_history.json')\n",
        "\n",
        "# Print summary\n",
        "visualizer.print_summary()\n",
        "\n",
        "# Plot training progress\n",
        "visualizer.plot_training_progress(save_path='cifar100_training_progress.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üëÅÔ∏è Part 3: Attention Visualization\n",
        "\n",
        "Understand what regions the Vision Transformer focuses on when making predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Attention Extraction Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_attention_maps(model, image, device='cpu'):\n",
        "    \"\"\"Extract attention maps from all transformer blocks\"\"\"\n",
        "    model.eval()\n",
        "    attention_maps = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        x = image.to(device)\n",
        "        B = x.shape[0]\n",
        "        \n",
        "        # Patch embedding\n",
        "        x = model.patch_embed(x)\n",
        "        cls_tokens = model.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x = x + model.pos_embed\n",
        "        x = model.pos_dropout(x)\n",
        "        \n",
        "        # Extract attention from each block\n",
        "        for block in model.blocks:\n",
        "            x_norm = block.norm1(x)\n",
        "            B, N, C = x_norm.shape\n",
        "            attn_module = block.attn\n",
        "            \n",
        "            # Compute attention weights manually\n",
        "            qkv = attn_module.qkv(x_norm).reshape(B, N, 3, attn_module.num_heads, attn_module.head_dim)\n",
        "            qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "            \n",
        "            attn = (q @ k.transpose(-2, -1)) * attn_module.scale\n",
        "            attn = attn.softmax(dim=-1)\n",
        "            attention_maps.append(attn.detach().cpu())\n",
        "            \n",
        "            # Complete block forward\n",
        "            attn_dropped = attn_module.attn_dropout(attn)\n",
        "            x_attn = (attn_dropped @ v).transpose(1, 2).reshape(B, N, C)\n",
        "            x_attn = attn_module.proj(x_attn)\n",
        "            x_attn = attn_module.proj_dropout(x_attn)\n",
        "            x = x + x_attn\n",
        "            x = x + block.mlp(block.norm2(x))\n",
        "    \n",
        "    return attention_maps\n",
        "\n",
        "print(\"Attention extraction function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Visualize Layer-wise Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_attention(model, image_tensor, label, img_size=224, patch_size=16, save_path=None):\n",
        "    \"\"\"Visualize attention maps across layers\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    image_batch = image_tensor.unsqueeze(0)\n",
        "    \n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(image_batch.to(device))\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        pred_idx = torch.argmax(probs, dim=1).item()\n",
        "        confidence = probs[0, pred_idx].item()\n",
        "    \n",
        "    # Denormalize image\n",
        "    img_display = image_tensor.cpu().numpy().transpose(1, 2, 0)\n",
        "    mean = np.array([0.5070751592371323, 0.48654887331495095, 0.4409178433670343])\n",
        "    std = np.array([0.2673342858792401, 0.2564384629170883, 0.27615047132568404])\n",
        "    img_display = img_display * std + mean\n",
        "    img_display = np.clip(img_display, 0, 1)\n",
        "    \n",
        "    # Get attention maps\n",
        "    attention_maps = get_attention_maps(model, image_batch, device)\n",
        "    num_patches = (img_size // patch_size) ** 2\n",
        "    selected_layers = [0, len(attention_maps)//2, len(attention_maps)-1]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, len(selected_layers) + 1, figsize=(15, 8))\n",
        "    \n",
        "    # Show original image\n",
        "    axes[0, 0].imshow(img_display)\n",
        "    axes[0, 0].set_title(f'Original\\\\nTrue: {CIFAR100_CLASSES[label]}', fontsize=10)\n",
        "    axes[0, 0].axis('off')\n",
        "    \n",
        "    axes[1, 0].imshow(img_display)\n",
        "    pred_color = 'green' if pred_idx == label else 'red'\n",
        "    axes[1, 0].set_title(f'Prediction\\\\n{CIFAR100_CLASSES[pred_idx]} ({confidence*100:.1f}%)', \n",
        "                         fontsize=10, color=pred_color)\n",
        "    axes[1, 0].axis('off')\n",
        "    \n",
        "    # Visualize selected layers\n",
        "    for idx, layer_idx in enumerate(selected_layers, start=1):\n",
        "        attn = attention_maps[layer_idx][0]\n",
        "        attn_mean = attn.mean(dim=0)\n",
        "        cls_attn = attn_mean[0, 1:]\n",
        "        \n",
        "        grid_size = int(np.sqrt(num_patches))\n",
        "        cls_attn_map = cls_attn.reshape(grid_size, grid_size).numpy()\n",
        "        cls_attn_upsampled = np.kron(cls_attn_map, np.ones((patch_size, patch_size)))\n",
        "        \n",
        "        # Heatmap\n",
        "        im1 = axes[0, idx].imshow(cls_attn_upsampled, cmap='hot', interpolation='bilinear')\n",
        "        axes[0, idx].set_title(f'Layer {layer_idx+1}\\\\nHeatmap', fontsize=10)\n",
        "        axes[0, idx].axis('off')\n",
        "        plt.colorbar(im1, ax=axes[0, idx], fraction=0.046)\n",
        "        \n",
        "        # Overlay\n",
        "        axes[1, idx].imshow(img_display)\n",
        "        axes[1, idx].imshow(cls_attn_upsampled, cmap='hot', alpha=0.6, interpolation='bilinear')\n",
        "        axes[1, idx].set_title(f'Layer {layer_idx+1}\\\\nOverlay', fontsize=10)\n",
        "        axes[1, idx].axis('off')\n",
        "    \n",
        "    plt.suptitle('ViT Attention Visualization - Different Layers Focus on Different Regions', \n",
        "                 fontsize=14, y=0.98)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Saved: {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"Layer-wise attention visualization function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Run Attention Visualization on Sample Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained model\n",
        "model, checkpoint = load_model('./src/checkpoints/best_model.pth', device=device)\n",
        "print(f\"Loaded CIFAR-100 model - Best Acc: {checkpoint['best_acc']:.2f}%\")\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = CIFAR100Dataset(\n",
        "    './cifar-100-python/cifar-100-python',\n",
        "    train=False,\n",
        "    transform=get_transforms(224, train=False)\n",
        ")\n",
        "\n",
        "# Visualize attention for 2 random samples\n",
        "for i in range(2):\n",
        "    idx = np.random.randint(len(test_dataset))\n",
        "    image, label = test_dataset[idx]\n",
        "    print(f\"\\nExample {i+1}: {CIFAR100_CLASSES[label]}\")\n",
        "    visualize_attention(model, image, label, save_path=f'attention_example_{i+1}.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üîÑ Part 4: Transfer Learning - Fine-tuning on CIFAR-10\n",
        "\n",
        "Transfer the CIFAR-100 trained model to CIFAR-10 using Option A: Unfreeze Last 4 Transformer Blocks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 CIFAR-10 Dataset and Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CIFAR10Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"CIFAR-10 Dataset\"\"\"\n",
        "    def __init__(self, data_dir, train=True, transform=None):\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        \n",
        "        if train:\n",
        "            for i in range(1, 6):\n",
        "                with open(os.path.join(data_dir, f'data_batch_{i}'), 'rb') as f:\n",
        "                    batch = pickle.load(f, encoding='bytes')\n",
        "                    self.data.append(batch[b'data'])\n",
        "                    self.labels.extend(batch[b'labels'])\n",
        "            self.data = np.concatenate(self.data)\n",
        "        else:\n",
        "            with open(os.path.join(data_dir, 'test_batch'), 'rb') as f:\n",
        "                batch = pickle.load(f, encoding='bytes')\n",
        "                self.data = batch[b'data']\n",
        "                self.labels = batch[b'labels']\n",
        "        \n",
        "        self.data = self.data.reshape(-1, 3, 32, 32)\n",
        "        print(f\"Loaded {'train' if train else 'test'} data: {len(self.data)} images\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.from_numpy(self.data[idx]).float()\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "def get_cifar10_transforms(img_size=224, train=True):\n",
        "    \"\"\"CRITICAL: Use CIFAR-100 normalization (backbone expects this!)\"\"\"\n",
        "    mean = [0.5070751592371323, 0.48654887331495095, 0.4409178433670343]\n",
        "    std = [0.2673342858792401, 0.2564384629170883, 0.27615047132568404]\n",
        "    \n",
        "    if train:\n",
        "        return transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ConvertImageDtype(torch.float32),\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.ConvertImageDtype(torch.float32),\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "\n",
        "\n",
        "def load_cifar100_backbone(checkpoint_path, device):\n",
        "    \"\"\"Load CIFAR-100 model and adapt for CIFAR-10\"\"\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    \n",
        "    model_c100 = vit_small_patch16_224(num_classes=100)\n",
        "    model_c100.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    model_c10 = vit_small_patch16_224(num_classes=10)\n",
        "    \n",
        "    # Copy all weights except classification head\n",
        "    with torch.no_grad():\n",
        "        model_c10.patch_embed.load_state_dict(model_c100.patch_embed.state_dict())\n",
        "        model_c10.cls_token.copy_(model_c100.cls_token)\n",
        "        model_c10.pos_embed.copy_(model_c100.pos_embed)\n",
        "        for i in range(len(model_c10.blocks)):\n",
        "            model_c10.blocks[i].load_state_dict(model_c100.blocks[i].state_dict())\n",
        "        model_c10.norm.load_state_dict(model_c100.norm.state_dict())\n",
        "    \n",
        "    print(\"[OK] Loaded CIFAR-100 backbone, initialized 10-class head\")\n",
        "    return model_c10.to(device)\n",
        "\n",
        "\n",
        "def freeze_layers(model, unfreeze_last_n_blocks=4):\n",
        "    \"\"\"Freeze all except last N blocks and head\"\"\"\n",
        "    # Freeze everything\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Unfreeze last N blocks\n",
        "    for i in range(12 - unfreeze_last_n_blocks, 12):\n",
        "        for param in model.blocks[i].parameters():\n",
        "            param.requires_grad = True\n",
        "    \n",
        "    # Unfreeze norm and head\n",
        "    for param in model.norm.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.head.parameters():\n",
        "        param.requires_grad = True\n",
        "    \n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
        "    return model\n",
        "\n",
        "print(\"Transfer learning functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Load Model and Data (Run this cell to start fine-tuning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CIFAR-10 data\n",
        "train_dataset = CIFAR10Dataset('./cifar-10-python/cifar-10-batches-py', \n",
        "                               train=True, transform=get_cifar10_transforms(224, True))\n",
        "test_dataset = CIFAR10Dataset('./cifar-10-python/cifar-10-batches-py', \n",
        "                              train=False, transform=get_cifar10_transforms(224, False))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Load pretrained CIFAR-100 backbone\n",
        "ft_model = load_cifar100_backbone('./src/checkpoints/best_model.pth', device)\n",
        "ft_model = freeze_layers(ft_model, unfreeze_last_n_blocks=4)\n",
        "\n",
        "print(\"\\nModel ready for fine-tuning!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Fine-tune (Optional - Skip if model already trained)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to train (takes ~2 hours on GPU)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.AdamW(filter(lambda p: p.requires_grad, ft_model.parameters()), \n",
        "#                         lr=1e-4, weight_decay=0.01)\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
        "# \n",
        "# best_acc = 0.0\n",
        "# for epoch in range(1, 51):\n",
        "#     # Train\n",
        "#     ft_model.train()\n",
        "#     for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = ft_model(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#     \n",
        "#     # Validate\n",
        "#     ft_model.eval()\n",
        "#     correct, total = 0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for images, labels in test_loader:\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = ft_model(images)\n",
        "#             _, predicted = outputs.max(1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += predicted.eq(labels).sum().item()\n",
        "#     \n",
        "#     val_acc = 100. * correct / total\n",
        "#     print(f\"Epoch {epoch}: Val Acc = {val_acc:.2f}%\")\n",
        "#     \n",
        "#     if val_acc > best_acc:\n",
        "#         best_acc = val_acc\n",
        "#         torch.save({'model_state_dict': ft_model.state_dict(), 'best_acc': best_acc},\n",
        "#                   './src/cifar10_finetuned_optionA/best_model.pth')\n",
        "#     \n",
        "#     scheduler.step()\n",
        "\n",
        "print(\"To train: uncomment the code above and run this cell\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìä Part 5: Results Summary\n",
        "\n",
        "### Key Results from Our Implementation:\n",
        "\n",
        "| Model | Dataset | Method | Accuracy | Notes |\n",
        "|-------|---------|--------|----------|-------|\n",
        "| ViT-Small | CIFAR-100 | From Scratch | **66.65%** | 300 epochs, cosine LR |\n",
        "| ViT-Small | CIFAR-10 | Transfer (Frozen) | 12.21% | Only head unfrozen ‚ùå |\n",
        "| ViT-Small | CIFAR-10 | Transfer (Option A) | **63.66%** | Last 4 blocks unfrozen ‚úì |\n",
        "\n",
        "### Key Insights:\n",
        "\n",
        "1. **Training from Scratch**\n",
        "   - Successfully trained ViT-Small on CIFAR-100\n",
        "   - Achieved competitive 66.65% accuracy\n",
        "   - Used warmup + cosine LR schedule, RandAugment, gradient clipping\n",
        "\n",
        "2. **Attention Visualization**\n",
        "   - Early layers: Broad, distributed attention\n",
        "   - Middle layers: Feature aggregation  \n",
        "   - Late layers: Focused on discriminative regions\n",
        "\n",
        "3. **Transfer Learning**\n",
        "   - Freezing only head: **Failed** (12.21%)\n",
        "   - Unfreezing last 4 blocks: **Success** (63.66%)\n",
        "   - Top-5 accuracy: 96.75%\n",
        "   - **Lesson**: Strategic layer unfreezing is crucial!\n",
        "\n",
        "4. **Implementation Details**\n",
        "   - Must use CIFAR-100 normalization for transfer learning\n",
        "   - Lower learning rate for fine-tuning (1e-4 vs 1e-3)\n",
        "   - Fewer epochs needed (50 vs 300)\n",
        "\n",
        "---\n",
        "\n",
        "### üìö References:\n",
        "- **Paper**: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
        "- **Dataset**: [CIFAR-10/100](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "\n",
        "---\n",
        "\n",
        "**COMP3314 2025-2026 Group 16**\n",
        "\n",
        "*This notebook demonstrates the full pipeline: training, visualization, and transfer learning with Vision Transformers*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
